{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fbbda61-8a45-4ef9-a75a-49bdfc3808de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "data_train = pd.read_csv(\"data_train.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a087644c-4683-44db-8813-3dbc785323f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Consigne : si un mot du tweet, n'est pas présent dans le vocabulaire, on ne le prend pas le mot en compte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "aecbd828-0aa9-423f-941f-f6a97e1844ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class naives_bayes():\n",
    "    \n",
    "    def __init__(self, data, variante1, variante2, variante3):\n",
    "        \"\"\"\n",
    "        variante1 : taper \"fréquence\" ou \"présence\"\n",
    "        variante2 : mots sans importance (longueur<3), taper \"avec\" ou \"sans\"\n",
    "        variante3 : taper \"uni-gramme\", \"bi-gramme\", \"both\"\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.variante1 = variante1\n",
    "        self.variante2 = variante2\n",
    "        self.variante3 = variante3\n",
    "        self.p_0 = self.data[\"target\"].value_counts()[0]/self.data.shape[0]\n",
    "        self.p_2 = self.data[\"target\"].value_counts()[2]/self.data.shape[0]\n",
    "        self.p_4 = self.data[\"target\"].value_counts()[4]/self.data.shape[0]\n",
    "        self.dictionnaire_0, self.dictionnaire_2, self.dictionnaire_4 = self.dictionnaire()  \n",
    "        \n",
    "    def dictionnaire(self): # on crée le dictionnaire des mots et de leur occurence dans la database d'entraînement\n",
    "        dictionnaire_0 = {}\n",
    "        dictionnaire_2 = {}\n",
    "        dictionnaire_4 = {}\n",
    "        \n",
    "        \n",
    "        if self.variante3 == \"uni-gramme\" or self.variante3 == \"both\":\n",
    "            if self.variante2 == \"avec\":\n",
    "                for i in range(0,self.data.shape[0]):\n",
    "                    if self.data[\"target\"][i] == 0:\n",
    "                        for y in (\" \".join(literal_eval(self.data[\"Tweet_Tokenized\"][i]))).split(): #join et literal_eval pour transformer les series en liste de mot. Puis .split pour découper pour chaque mot\n",
    "                            if y not in dictionnaire_0 : \n",
    "                                dictionnaire_0[y] = 1\n",
    "                            else :\n",
    "                                dictionnaire_0[y] += 1\n",
    "\n",
    "                    if self.data[\"target\"][i] == 2:\n",
    "                        for y in (\" \".join(literal_eval(self.data[\"Tweet_Tokenized\"][i]))).split(): #join et literal_eval pour transformer les series en liste de mot. Puis .split pour découper pour chaque mot\n",
    "                            if y not in dictionnaire_2 : \n",
    "                                dictionnaire_2[y] = 1\n",
    "                            else :\n",
    "                                dictionnaire_2[y] += 1\n",
    "\n",
    "                    if self.data[\"target\"][i] == 4:\n",
    "                        for y in (\" \".join(literal_eval(self.data[\"Tweet_Tokenized\"][i]))).split(): #join et literal_eval pour transformer les series en liste de mot. Puis .split pour découper pour chaque mot\n",
    "                            if y not in dictionnaire_4 : \n",
    "                                dictionnaire_4[y] = 1\n",
    "                            else :\n",
    "                                dictionnaire_4[y] += 1\n",
    "                                \n",
    "        \n",
    "            if self.variante2 == \"sans\":\n",
    "                for i in range(0,self.data.shape[0]):\n",
    "                    if self.data[\"target\"][i] == 0:\n",
    "                        for y in (\" \".join(literal_eval(self.data[\"Tweet_Tokenized\"][i]))).split(): #join et literal_eval pour transformer les series en liste de mot. Puis .split pour découper pour chaque mot\n",
    "                            if y not in dictionnaire_0 and len(y)>3 : \n",
    "                                dictionnaire_0[y] = 1\n",
    "                            elif len(y)>3 :\n",
    "                                dictionnaire_0[y] += 1\n",
    "\n",
    "                    if self.data[\"target\"][i] == 2:\n",
    "                        for y in (\" \".join(literal_eval(self.data[\"Tweet_Tokenized\"][i]))).split(): #join et literal_eval pour transformer les series en liste de mot. Puis .split pour découper pour chaque mot\n",
    "                            if y not in dictionnaire_2 and len(y)>3  : \n",
    "                                dictionnaire_2[y] = 1\n",
    "                            elif len(y)>3  :\n",
    "                                dictionnaire_2[y] += 1\n",
    "\n",
    "                    if self.data[\"target\"][i] == 4:\n",
    "                        for y in (\" \".join(literal_eval(self.data[\"Tweet_Tokenized\"][i]))).split(): #join et literal_eval pour transformer les series en liste de mot. Puis .split pour découper pour chaque mot\n",
    "                            if y not in dictionnaire_4 and len(y)>3 : \n",
    "                                dictionnaire_4[y] = 1\n",
    "                            elif len(y)>3  :\n",
    "                                dictionnaire_4[y] += 1\n",
    "                                \n",
    "                                \n",
    "        if self.variante3 == \"bi-gramme\" or self.variante3 == \"both\":\n",
    "            if self.variante2 == \"avec\":\n",
    "                for i in range(0,self.data.shape[0]):\n",
    "                    if self.data[\"target\"][i] == 0:\n",
    "                        mots = (\" \".join(literal_eval(self.data[\"Tweet_Tokenized\"][i]))).split()\n",
    "                        for y in range(0,len(mots)-1):\n",
    "                            if mots[y]+\" \"+mots[y+1] not in dictionnaire_0 : \n",
    "                                dictionnaire_0[mots[y]+\" \"+mots[y+1]] = 1\n",
    "                            else :\n",
    "                                dictionnaire_0[mots[y]+\" \"+mots[y+1]] += 1\n",
    "\n",
    "\n",
    "                    if self.data[\"target\"][i] == 2:\n",
    "                        mots = (\" \".join(literal_eval(self.data[\"Tweet_Tokenized\"][i]))).split()\n",
    "                        for y in range(0,len(mots)-1):\n",
    "                            if mots[y]+\" \"+mots[y+1] not in dictionnaire_2 : \n",
    "                                dictionnaire_2[mots[y]+\" \"+mots[y+1]] = 1\n",
    "                            else :\n",
    "                                dictionnaire_2[mots[y]+\" \"+mots[y+1]] += 1\n",
    "\n",
    "                    if self.data[\"target\"][i] == 4:\n",
    "                        mots = (\" \".join(literal_eval(self.data[\"Tweet_Tokenized\"][i]))).split()\n",
    "                        for y in range(0,len(mots)-1):\n",
    "                            if mots[y]+\" \"+mots[y+1] not in dictionnaire_4 : \n",
    "                                dictionnaire_4[mots[y]+\" \"+mots[y+1]] = 1\n",
    "                            else :\n",
    "                                dictionnaire_4[mots[y]+\" \"+mots[y+1]] += 1\n",
    "                                \n",
    "        \n",
    "            if self.variante2 == \"sans\":\n",
    "                for i in range(0,self.data.shape[0]):\n",
    "                    if self.data[\"target\"][i] == 0:\n",
    "                        mots = (\" \".join(literal_eval(self.data[\"Tweet_Tokenized\"][i]))).split()\n",
    "                        for y in range(0,len(mots)-1):\n",
    "                            if mots[y]+\" \"+mots[y+1] not in dictionnaire_0 and len(mots[y])>3 and len(mots[y+1])>3: \n",
    "                                dictionnaire_0[mots[y]+\" \"+mots[y+1]] = 1\n",
    "                            elif len(mots[y])>3 and len(mots[y+1])>3:\n",
    "                                dictionnaire_0[mots[y]+\" \"+mots[y+1]] += 1\n",
    "\n",
    "\n",
    "                    if self.data[\"target\"][i] == 2:\n",
    "                        mots = (\" \".join(literal_eval(self.data[\"Tweet_Tokenized\"][i]))).split()\n",
    "                        for y in range(0,len(mots)-1):\n",
    "                            if mots[y]+\" \"+mots[y+1] not in dictionnaire_2 and len(mots[y])>3 and len(mots[y+1])>3: \n",
    "                                dictionnaire_2[mots[y]+\" \"+mots[y+1]] = 1\n",
    "                            elif len(mots[y])>3 and len(mots[y+1])>3:\n",
    "                                dictionnaire_2[mots[y]+\" \"+mots[y+1]] += 1\n",
    "\n",
    "                    if self.data[\"target\"][i] == 4:\n",
    "                        mots = (\" \".join(literal_eval(self.data[\"Tweet_Tokenized\"][i]))).split()\n",
    "                        for y in range(0,len(mots)-1):\n",
    "                            if mots[y]+\" \"+mots[y+1] not in dictionnaire_4 and len(mots[y])>3 and len(mots[y+1])>3: \n",
    "                                dictionnaire_4[mots[y]+\" \"+mots[y+1]] = 1\n",
    "                            elif len(mots[y])>3 and len(mots[y+1])>3:\n",
    "                                dictionnaire_4[mots[y]+\" \"+mots[y+1]] += 1\n",
    "\n",
    "\n",
    "        return dictionnaire_0,dictionnaire_2,dictionnaire_4\n",
    "\n",
    "                        \n",
    "    def classification(self, tweet_a_categoriser):\n",
    "        mots_tweet_a_categoriser = tweet_a_categoriser.split()\n",
    "        n_0 = np.array(list(self.dictionnaire_0.values())).sum() # nombre de mot contenus dans les tweets 0\n",
    "        n_2 = np.array(list(self.dictionnaire_2.values())).sum() # nombre de mot contenus dans les tweets 2\n",
    "        n_4 = np.array(list(self.dictionnaire_4.values())).sum() # nombre de mot contenus dans les tweets 4\n",
    "        N = n_0 + n_2 + n_4\n",
    "        n_0_N = n_0 + N\n",
    "        n_2_N = n_2 + N\n",
    "        n_4_N = n_4 + N\n",
    "        \n",
    "        n_t_0 = 1\n",
    "        n_t_2 = 1\n",
    "        n_t_4 = 1\n",
    "        p_t_0 = 1\n",
    "        p_t_2 = 1\n",
    "        p_t_4 = 1\n",
    "        \n",
    "        if self.variante1 == \"présence\":\n",
    "            \n",
    "            for i in mots_tweet_a_categoriser:\n",
    "                if i in self.dictionnaire_0: \n",
    "                    n_t_0 = (self.dictionnaire_0[i] + 1)\n",
    "                else:\n",
    "                    n_t_0 = 1\n",
    "                p_t_0 *= n_t_0/n_0_N\n",
    "\n",
    "\n",
    "                if i in self.dictionnaire_2: \n",
    "                    n_t_2 = (self.dictionnaire_2[i] + 1)\n",
    "                else:\n",
    "                    n_t_2 = 1\n",
    "                p_t_2 *= n_t_2/n_2_N\n",
    "\n",
    "\n",
    "                if i in self.dictionnaire_4: \n",
    "                    n_t_4 = (self.dictionnaire_4[i] + 1)\n",
    "                else:\n",
    "                    n_t_4 = 1\n",
    "                p_t_4 *= n_t_4/n_4_N\n",
    "        \n",
    "        \n",
    "        if self.variante1 == \"fréquence\":\n",
    "            occurence_mots_tweet = Counter(mots_tweet_a_categoriser)\n",
    "            \n",
    "            for i in occurence_mots_tweet:\n",
    "                if i in self.dictionnaire_0: \n",
    "                    n_t_0 = (self.dictionnaire_0[i] + 1)\n",
    "                else:\n",
    "                    n_t_0 = 1\n",
    "                p_t_0 *= (n_t_0/n_0_N) * occurence_mots_tweet[i]\n",
    "\n",
    "\n",
    "                if i in self.dictionnaire_2: \n",
    "                    n_t_2 = (self.dictionnaire_2[i] + 1)\n",
    "                else:\n",
    "                    n_t_2 = 1\n",
    "                p_t_2 *= (n_t_2/n_2_N) * occurence_mots_tweet[i]\n",
    "\n",
    "\n",
    "                if i in self.dictionnaire_4: \n",
    "                    n_t_4 = (self.dictionnaire_4[i] + 1)\n",
    "                else:\n",
    "                    n_t_4 = 1\n",
    "                p_t_4 *= (n_t_4/n_4_N) * occurence_mots_tweet[i]\n",
    "\n",
    "\n",
    "        p_0_t = p_t_0 * self.p_0\n",
    "        p_2_t = p_t_2 * self.p_2\n",
    "        p_4_t = p_t_4 * self.p_4\n",
    "\n",
    "        probas = [p_0_t, p_2_t, p_4_t]\n",
    "        labels = [\"négatif\", \"neutre\", \"positif\"]\n",
    "\n",
    "        label_pred = labels[probas.index(max(probas))]\n",
    "\n",
    "        return {\"négatif\":p_0_t, \"neutre\":p_2_t, \"positif\":p_4_t}, \"l'étiquette retenue est\"+\" \"+label_pred\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e36af314-4fd1-4c27-9950-19935a921dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'négatif': 3.3019829081963677e-11,\n",
       "  'neutre': 2.01526381013024e-12,\n",
       "  'positif': 1.9907975501658523e-12},\n",
       " \"l'étiquette retenue est négatif\")"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naives_bayes(data_train, \"fréquence\", \"sans\", \"uni-gramme\").classification(\"I hate it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a7f92ba7-078d-4a8b-9d94-ef99e34c3217",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'négatif': 8.653426181144893e-09,\n",
       "  'neutre': 4.7901238677642844e-11,\n",
       "  'positif': 8.037669325110847e-10},\n",
       " \"l'étiquette retenue est négatif\")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naives_bayes(data_train, \"fréquence\", \"avec\", \"uni-gramme\").classification(\"I hate it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c984978a-b04e-4953-a479-f6c6669620cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'négatif': 1.6185293027421837e-11,\n",
       "  'neutre': 1.5892192257228137e-11,\n",
       "  'positif': 1.6131953246010665e-11},\n",
       " \"l'étiquette retenue est négatif\")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naives_bayes(data_train, \"fréquence\", \"sans\", \"bi-gramme\").classification(\"I hate it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "27750e9c-ea99-4606-a9eb-6a5e51a4c1a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'négatif': 4.498271908480641e-13,\n",
       "  'neutre': 4.965336676685019e-13,\n",
       "  'positif': 4.684272702658437e-13},\n",
       " \"l'étiquette retenue est neutre\")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naives_bayes(data_train, \"fréquence\", \"avec\", \"bi-gramme\").classification(\"I hate it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8db04ea0-e6b2-40f2-b849-749087d43fca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'négatif': 1.1975535921036108e-09,\n",
       "  'neutre': 6.664792430102816e-12,\n",
       "  'positif': 1.1139307638514731e-10},\n",
       " \"l'étiquette retenue est négatif\")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naives_bayes(data_train, \"fréquence\", \"avec\", \"both\").classification(\"I hate it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "230d8bb4-26ef-4042-9abd-2f520e6fdcdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'négatif': 9.916899509934244e-12,\n",
       "  'neutre': 5.942585617998775e-13,\n",
       "  'positif': 5.923894280318838e-13},\n",
       " \"l'étiquette retenue est négatif\")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naives_bayes(data_train, \"fréquence\", \"sans\", \"both\").classification(\"I hate it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37fd08f-45ad-47a2-bab6-fee4762a4b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "25dd029c-1882-4507-9f3a-adf6511652f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'négatif': 2.036100277916445e-09,\n",
       "  'neutre': 4.7901238677642844e-11,\n",
       "  'positif': 1.5271571717710612e-08},\n",
       " \"l'étiquette retenue est positif\")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naives_bayes(data_train, \"présence\", \"avec\", \"uni-gramme\").classification(\"I love it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0fc1b042-e9eb-4ccb-be90-e22d87ad0f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'négatif': 4.498271908480641e-13,\n",
       "  'neutre': 4.965336676685019e-13,\n",
       "  'positif': 4.684272702658437e-13},\n",
       " \"l'étiquette retenue est neutre\")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naives_bayes(data_train, \"présence\", \"avec\", \"bi-gramme\").classification(\"I love it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "05a964cd-8fa9-4c95-a3de-90de15de517e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'négatif': 2.817773157890849e-10,\n",
       "  'neutre': 6.664792430102816e-12,\n",
       "  'positif': 2.116468451317799e-09},\n",
       " \"l'étiquette retenue est positif\")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naives_bayes(data_train, \"présence\", \"avec\", \"both\").classification(\"I love it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "32a1bbf5-8996-4049-a0b7-6e1b7d6db2c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'négatif': 7.769371548697338e-12,\n",
       "  'neutre': 2.01526381013024e-12,\n",
       "  'positif': 3.782515345315118e-11},\n",
       " \"l'étiquette retenue est positif\")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naives_bayes(data_train, \"présence\", \"sans\", \"uni-gramme\").classification(\"I love it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "138c8236-0a61-4e42-bf2a-78e2bcff80e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'négatif': 1.6185293027421837e-11,\n",
       "  'neutre': 1.5892192257228137e-11,\n",
       "  'positif': 1.6131953246010665e-11},\n",
       " \"l'étiquette retenue est négatif\")"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naives_bayes(data_train, \"présence\", \"sans\", \"bi-gramme\").classification(\"I love it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a5dbf1be-4e30-4272-8ef9-994f3ee8d4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'négatif': 2.333388119984528e-12,\n",
       "  'neutre': 5.942585617998775e-13,\n",
       "  'positif': 1.1255399132605793e-11},\n",
       " \"l'étiquette retenue est positif\")"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naives_bayes(data_train, \"présence\", \"sans\", \"both\").classification(\"I love it\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
